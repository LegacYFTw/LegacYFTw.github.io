{"title":"Understanding The Chernoff Bound In The Context Of Machine Learning","markdown":{"yaml":{"title":"Understanding The Chernoff Bound In The Context Of Machine Learning","description":"Modelling uncertainty using concentration bounds in the context of machine learning.","author":"Turbasu Chatterjee","date":"11/14/2023","toc":true,"categories":["class-project","machine-learning","probability-theory"]},"headingText":"Theoretical Foundations","containsRefs":false,"markdown":"\n\n\n\nModelling uncertainty is one of the prime objectives of machine learning. In this blog post we will take a deep dive into methods to model this uncertainty using concentration inequalities, specifically the Chernoff bound and the Hoeffding bound. Further, we shall use a simple example to illustrate how good these bounds are at what they do.\n\n\nThe Chernoff inequality relies on moment generating functions (MGFs), which provides information on the distribution of random variables. The MGF of a random variable X is defined as:\n\n\\begin{align}\n M_x(t) &= \\mathbb{E}[e^{tX}] \\\\\n        &= 1 + t \\mathbb{E}[X] + \\frac{t^2 \\mathbb{E}[X^2]}{2!} + \\frac{t^3 \\mathbb{E}[X^3]}{3!} + \\dots \\\\\n        &= 1 + tm_1 + \\frac{tm_2}{2!} + \\frac{tm_3}{3!} + \\dots\n\\end{align}\n\nwhere, $m_1, m_2, \\dots $ are the first moment, second moment and so on, $X$ is a random variable and $\\mathbb{E}[X]$ is the expectation of that random variable.\n\nWe shall define a *threshold*, $a$ as a positive real number for which we want to find the probability $\\Pr(X \\geq a)$.\n\n\nFor positive $t$, this gives a bound on the right tail of $X$ in terms of its MGF. This is given by:\n\n\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t > 0.\n\\end{equation}\n\nNow since this bound holds for for every $t>0$, we can take the infimum as follows:\n\n\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t > 0} M(t) e^{-ta}.\n\\end{equation}\n\n\n\nWe can obtain a bound on the right tail by performing similar analyses using $t < 0$.\n\n\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t<0.\n\\end{equation}\n\nAgain, taking the infimum, we have\n\n\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t < 0} M(t) e^{-ta}.\n\\end{equation}\n\n\n\n\n\n\n\n## An Example Using Coin Flipping\n\nIn this scenario, we shall use the idea of simulating tossing biased coins. Lets begin this by calling our imports.\n\nNow we shall generate our dataset using three biased coins. We shall do this by constructing a function that takes in the number of coin flips and the bias of the coin. Next we will visualize how the cumulative head probability changes over time, showing the law of large numbers in action.\n\nNow we shall try to estimate how the cumulative probability of getting a head shifts deviates significantly from the true bias. Here, we shall employ the Chernoff bound that calculates the probability of the cumulative head probability deviating significantly from the true bias.\n\nIn order to do this, we emply a function that calculates the Chernoff bound as below:\n\nNow we shall set our value of $\\varepsilon$ which will serve as the error threshold. We plot a visualization to show how the Chernoff Inequality bounds (in blue) change as the number of coin tosses increases. The red dashed line represents the chosen error threshold $\\varepsilon$, and we can observe how the bounds tighten with more data.\n\nBy applying the Chernoff bound to practical examples, such as simulating biased coin tosses, and using visualizations, we can gain a deeper understanding of how this inequality works and how it allows us to set confidence bounds on real-world data.\n\nWe shall see how the Chernoff bound how the Chernoff bound plays an important role in machine learning research in the subsequent section.\n\n## Using The Chernoff Bound in Machine Learning\n\nSuppose the task at hand is to build and understand a credit card fraud detection model. We shall deem that credit card fraud detection is a rare event and we want to be able to stop misclassifying such events using the Chernoff bound.\n\nIn order to do this, we shall generate our own dataset for this example, such that we have two classes: \"Genuine\" and \"Fraudulent\" transactions. In order to signify the rarity of fraudulent events, we shall grossly limit the representation of these events by just 5% of the total data.\n\n\nWe will now create a label by assigning a ```0``` to the 950 real transactions and a ```1``` to the 50 fraudulent transactions. We then split the training and the testing data by using 20% of the data generated for verification and testing. We than use a Random Forest Classifier to train and classify our model. Then we evaluate our classification model use an accuracy metric.\n\nThe model accuracy comes out to 94.5%. However, there is more than what meets the eye. In order to see this, we must first visualize the dataset that we have generated.\n\nNow we shall apply the Chernoff bound to estimate the probability of our Random Forest Classifier misclassifying a fraudulent transaction as a genuine one.\n\nThe Chernoff probability i.e. the probability of datapoints being misclassified in our little experiment comes out to around **94%**, which should be alarming, despite what our model accuracy metric tells us.\n\nIn order to calculate how the Chernoff probability changes as sample size increases, we have the following piece of code:\n\nFinally we plot how this model performs against a confusion matrix.\n\nIn conclusion, we have seen how the performance metric does not give us an idea for the true performance of a classifier. The performance of this model can be better understood by the use of the Chernoff concentration bound. Through visualizations, we illustrated the data distribution, Chernoff probabilities, and model performance. Understanding and applying the Chernoff Inequality can help assess the reliability of machine learning models when dealing with rare events.\n","srcMarkdownNoYaml":"\n\n\n\nModelling uncertainty is one of the prime objectives of machine learning. In this blog post we will take a deep dive into methods to model this uncertainty using concentration inequalities, specifically the Chernoff bound and the Hoeffding bound. Further, we shall use a simple example to illustrate how good these bounds are at what they do.\n\n## Theoretical Foundations\n\nThe Chernoff inequality relies on moment generating functions (MGFs), which provides information on the distribution of random variables. The MGF of a random variable X is defined as:\n\n\\begin{align}\n M_x(t) &= \\mathbb{E}[e^{tX}] \\\\\n        &= 1 + t \\mathbb{E}[X] + \\frac{t^2 \\mathbb{E}[X^2]}{2!} + \\frac{t^3 \\mathbb{E}[X^3]}{3!} + \\dots \\\\\n        &= 1 + tm_1 + \\frac{tm_2}{2!} + \\frac{tm_3}{3!} + \\dots\n\\end{align}\n\nwhere, $m_1, m_2, \\dots $ are the first moment, second moment and so on, $X$ is a random variable and $\\mathbb{E}[X]$ is the expectation of that random variable.\n\nWe shall define a *threshold*, $a$ as a positive real number for which we want to find the probability $\\Pr(X \\geq a)$.\n\n\nFor positive $t$, this gives a bound on the right tail of $X$ in terms of its MGF. This is given by:\n\n\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t > 0.\n\\end{equation}\n\nNow since this bound holds for for every $t>0$, we can take the infimum as follows:\n\n\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t > 0} M(t) e^{-ta}.\n\\end{equation}\n\n\n\nWe can obtain a bound on the right tail by performing similar analyses using $t < 0$.\n\n\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t<0.\n\\end{equation}\n\nAgain, taking the infimum, we have\n\n\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t < 0} M(t) e^{-ta}.\n\\end{equation}\n\n\n\n\n\n\n\n## An Example Using Coin Flipping\n\nIn this scenario, we shall use the idea of simulating tossing biased coins. Lets begin this by calling our imports.\n\nNow we shall generate our dataset using three biased coins. We shall do this by constructing a function that takes in the number of coin flips and the bias of the coin. Next we will visualize how the cumulative head probability changes over time, showing the law of large numbers in action.\n\nNow we shall try to estimate how the cumulative probability of getting a head shifts deviates significantly from the true bias. Here, we shall employ the Chernoff bound that calculates the probability of the cumulative head probability deviating significantly from the true bias.\n\nIn order to do this, we emply a function that calculates the Chernoff bound as below:\n\nNow we shall set our value of $\\varepsilon$ which will serve as the error threshold. We plot a visualization to show how the Chernoff Inequality bounds (in blue) change as the number of coin tosses increases. The red dashed line represents the chosen error threshold $\\varepsilon$, and we can observe how the bounds tighten with more data.\n\nBy applying the Chernoff bound to practical examples, such as simulating biased coin tosses, and using visualizations, we can gain a deeper understanding of how this inequality works and how it allows us to set confidence bounds on real-world data.\n\nWe shall see how the Chernoff bound how the Chernoff bound plays an important role in machine learning research in the subsequent section.\n\n## Using The Chernoff Bound in Machine Learning\n\nSuppose the task at hand is to build and understand a credit card fraud detection model. We shall deem that credit card fraud detection is a rare event and we want to be able to stop misclassifying such events using the Chernoff bound.\n\nIn order to do this, we shall generate our own dataset for this example, such that we have two classes: \"Genuine\" and \"Fraudulent\" transactions. In order to signify the rarity of fraudulent events, we shall grossly limit the representation of these events by just 5% of the total data.\n\n\nWe will now create a label by assigning a ```0``` to the 950 real transactions and a ```1``` to the 50 fraudulent transactions. We then split the training and the testing data by using 20% of the data generated for verification and testing. We than use a Random Forest Classifier to train and classify our model. Then we evaluate our classification model use an accuracy metric.\n\nThe model accuracy comes out to 94.5%. However, there is more than what meets the eye. In order to see this, we must first visualize the dataset that we have generated.\n\nNow we shall apply the Chernoff bound to estimate the probability of our Random Forest Classifier misclassifying a fraudulent transaction as a genuine one.\n\nThe Chernoff probability i.e. the probability of datapoints being misclassified in our little experiment comes out to around **94%**, which should be alarming, despite what our model accuracy metric tells us.\n\nIn order to calculate how the Chernoff probability changes as sample size increases, we have the following piece of code:\n\nFinally we plot how this model performs against a confusion matrix.\n\nIn conclusion, we have seen how the performance metric does not give us an idea for the true performance of a classifier. The performance of this model can be better understood by the use of the Chernoff concentration bound. Through visualizations, we illustrated the data distribution, Chernoff probabilities, and model performance. Understanding and applying the Chernoff Inequality can help assess the reliability of machine learning models when dealing with rare events.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Concentration_Bound.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"zephyr","title":"Understanding The Chernoff Bound In The Context Of Machine Learning","description":"Modelling uncertainty using concentration bounds in the context of machine learning.","author":"Turbasu Chatterjee","date":"11/14/2023","categories":["class-project","machine-learning","probability-theory"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}