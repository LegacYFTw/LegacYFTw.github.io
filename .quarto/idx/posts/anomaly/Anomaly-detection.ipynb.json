{"title":"Anomaly And Outlier Detection In Machine Learning","markdown":{"yaml":{"title":"Anomaly And Outlier Detection In Machine Learning","description":"An introduction to basic methodology in anomaly and outlier detection","author":"Turbasu Chatterjee","date":"11/14/2023","toc":true,"categories":["class-project","machine-learning","anomaly-detection","outlier-detection"]},"headingText":"The K-Nearest Neighbor Algorithm For Outlier Detection","containsRefs":false,"markdown":"\n\n\n\nIn the world of data analysis and machine learning, one of the most critical tasks is the detection of outliers and anomalies in data. In this blog post, we'll explore how to use machine learning techniques to identify outliers and anomalies in data and provide code examples using Python. However, before we do any of that, it is important to define the ideas of anomalies and outliers statistically. Turns out, this is pretty ambiguous, so we will try to clear them up here:\n\n- An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism, i.e. these are points that are inconsistent with the remainder of the dataset.\n- An anomaly is an instance of data that occur very rarely in the dataset and one whose features differ significantly from most of the data and do not conform to a well-defined notion of normal behaviour.\n\nIn this article, we shall employ two elementary methods for outlier detection: The K-Nearest Neighbors (KNN) algorithm and the Isolation Forest algorithm.\n\n\nIn the realm of supervised learning, the K-Nearest Neighbors (KNN) algorithm stands as a fundamental yet powerful method for classification and regression tasks. This algorithm, rooted in the concept of similarity, is intuitive, versatile, and widely employed across various domains. At its core, KNN is a non-parametric, lazy learning algorithm used for both classification and regression. It operates on the principle of proximity, where the class or value of an unseen data point is determined by the classes or values of its nearest neighbors in the training dataset.\n\nLet us take a closer look at the inner working of the KNN algorithm. There are 3 main components of the KNN algorithm that we need to learn before we progress:\n\n- For an unseen data point, the KNN computes its distance to all points in the training data. This distance maybe calculated using norms like euclidean norm, minkowski norm, manhattan norm, etc..\n- Now the algorithm shall identify the K closest data points based on the calculated distances.\n- It will then assign a class that is most common for the K neighbors.\n\nLet us put theory to practice and use the ```scikit-learn``` to identify anomalies in the ```iris``` dataset.\n\nLet us load the Iris dataset now.\n\nWe shall now only use numeric data to make it easier on ourselves.\n\nLets now invoke the KNN algorithm and set our ```k``` to be equal to 5 and fit the algorithm on the numeric dataset that we have now. We will set our ```k``` to be equal to 5 in our case.\n\nNow, lets calculate the distance to the ```k``` neighbors\n\nLet us see what the distances look like now:\n\nNow we calculate outlier scores based on the distance metric.\n\nNow, we shall set our threshold for the outliers. This can be adjusted accordingly, but for now, we shall keep our threshold at the 95th percentile.\n\nNow let's see what our outliers are.\n\nNow let's visualize the outliers that we have found so far:\n\n## The Isolation Forest Algorithm For Anomalies\n\nWe now use the Isolation Forest algorithm for classifying anomalies. Developed by Liu, Ting, and Zhou in 2008, the IsolationForest algorithm stands out for its ability to efficiently isolate anomalies within a dataset. Unlike traditional distance or density-based methods, IsolationForest utilizes a tree-based structure to identify anomalies based on their isolation from the majority of data points.\n\nThe key concepts used in the Isolation Forest algorithm are:\n\n- IsolationForest randomly selects features and splits data points, recursively creating isolation trees. This is called random partitioning.\n- Anomalies are expected to have shorter path lengths in the trees, resulting in fewer partitions to be isolated.\n\nThe methodology for the Isolation Forest is as follows: First we select features and split the data points until they are isolated or a specific tree length is reached. Now we calculate the average path length to isolate each data point across multiple trees. Then, as mentioned before, shorter path lengths are considered to be anomalies.\n\nNow, we shall put this theory into practice. Using the same dataset, we shall invoke an ```IsolationForest``` object to classify the numeric dataset.\n\nLets see what are the outliers in the dataset.\n\nNow lets visualize the results:\n\nHere darker shades represents higher outlier scores, which means they are definitely outliers.\n\nTherefore, we have seen how we can perform outlier detection in python with these elementary algorithms and we have visualized the effectiveness of the algorithms.\n","srcMarkdownNoYaml":"\n\n\n\nIn the world of data analysis and machine learning, one of the most critical tasks is the detection of outliers and anomalies in data. In this blog post, we'll explore how to use machine learning techniques to identify outliers and anomalies in data and provide code examples using Python. However, before we do any of that, it is important to define the ideas of anomalies and outliers statistically. Turns out, this is pretty ambiguous, so we will try to clear them up here:\n\n- An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism, i.e. these are points that are inconsistent with the remainder of the dataset.\n- An anomaly is an instance of data that occur very rarely in the dataset and one whose features differ significantly from most of the data and do not conform to a well-defined notion of normal behaviour.\n\nIn this article, we shall employ two elementary methods for outlier detection: The K-Nearest Neighbors (KNN) algorithm and the Isolation Forest algorithm.\n\n## The K-Nearest Neighbor Algorithm For Outlier Detection\n\nIn the realm of supervised learning, the K-Nearest Neighbors (KNN) algorithm stands as a fundamental yet powerful method for classification and regression tasks. This algorithm, rooted in the concept of similarity, is intuitive, versatile, and widely employed across various domains. At its core, KNN is a non-parametric, lazy learning algorithm used for both classification and regression. It operates on the principle of proximity, where the class or value of an unseen data point is determined by the classes or values of its nearest neighbors in the training dataset.\n\nLet us take a closer look at the inner working of the KNN algorithm. There are 3 main components of the KNN algorithm that we need to learn before we progress:\n\n- For an unseen data point, the KNN computes its distance to all points in the training data. This distance maybe calculated using norms like euclidean norm, minkowski norm, manhattan norm, etc..\n- Now the algorithm shall identify the K closest data points based on the calculated distances.\n- It will then assign a class that is most common for the K neighbors.\n\nLet us put theory to practice and use the ```scikit-learn``` to identify anomalies in the ```iris``` dataset.\n\nLet us load the Iris dataset now.\n\nWe shall now only use numeric data to make it easier on ourselves.\n\nLets now invoke the KNN algorithm and set our ```k``` to be equal to 5 and fit the algorithm on the numeric dataset that we have now. We will set our ```k``` to be equal to 5 in our case.\n\nNow, lets calculate the distance to the ```k``` neighbors\n\nLet us see what the distances look like now:\n\nNow we calculate outlier scores based on the distance metric.\n\nNow, we shall set our threshold for the outliers. This can be adjusted accordingly, but for now, we shall keep our threshold at the 95th percentile.\n\nNow let's see what our outliers are.\n\nNow let's visualize the outliers that we have found so far:\n\n## The Isolation Forest Algorithm For Anomalies\n\nWe now use the Isolation Forest algorithm for classifying anomalies. Developed by Liu, Ting, and Zhou in 2008, the IsolationForest algorithm stands out for its ability to efficiently isolate anomalies within a dataset. Unlike traditional distance or density-based methods, IsolationForest utilizes a tree-based structure to identify anomalies based on their isolation from the majority of data points.\n\nThe key concepts used in the Isolation Forest algorithm are:\n\n- IsolationForest randomly selects features and splits data points, recursively creating isolation trees. This is called random partitioning.\n- Anomalies are expected to have shorter path lengths in the trees, resulting in fewer partitions to be isolated.\n\nThe methodology for the Isolation Forest is as follows: First we select features and split the data points until they are isolated or a specific tree length is reached. Now we calculate the average path length to isolate each data point across multiple trees. Then, as mentioned before, shorter path lengths are considered to be anomalies.\n\nNow, we shall put this theory into practice. Using the same dataset, we shall invoke an ```IsolationForest``` object to classify the numeric dataset.\n\nLets see what are the outliers in the dataset.\n\nNow lets visualize the results:\n\nHere darker shades represents higher outlier scores, which means they are definitely outliers.\n\nTherefore, we have seen how we can perform outlier detection in python with these elementary algorithms and we have visualized the effectiveness of the algorithms.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Anomaly-detection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"zephyr","title":"Anomaly And Outlier Detection In Machine Learning","description":"An introduction to basic methodology in anomaly and outlier detection","author":"Turbasu Chatterjee","date":"11/14/2023","categories":["class-project","machine-learning","anomaly-detection","outlier-detection"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}