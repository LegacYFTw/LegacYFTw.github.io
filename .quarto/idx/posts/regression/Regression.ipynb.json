{"title":"Linear and Non-Linear Regression: A Performance Comparison","markdown":{"yaml":{"title":"Linear and Non-Linear Regression: A Performance Comparison","description":"An introduction and performance comparison of the linear and non-linear regression techniques.","author":"Turbasu Chatterjee","date":"11/14/2023","toc":true,"categories":["class-project","machine-learning","regression"]},"headingText":"Data Loading and EDA","containsRefs":false,"markdown":"\n\n\n\n\nRegression analysis is a basic yet powerful tool in the field of statistics and machine learning that allows us to probe in and model the the relationship between one or more independent variables and a dependent variable. Here, we shall take a look at linear and non-linear regression and the differences in performance.\n\nIn order to elucidate, we shall use the [USA Housing Prices dataset](https://www.kaggle.com/datasets/kanths028/usa-housing/) from Kaggle. However, before we get our hands dirty into the math and code, let us load our dataset and perform some exploratory data analysis.\n\n\nIn the python cells below, we shall call the necessary libraries to perform our analysis. Further, we shall first call our dataset and explore its characteristics to inspect it.\n\nLet us see now plot the data and see if there exists any pairwise relationship in our data.\n\n\n\n# A Linear Regression Model\n\nNow that we have inspected our data and have carried out our EDA, we shall begin to find ways to see if there exists any linear relationships between the data. Our primary focus of this dataset is the price. We need to see if there exists any relationship between the price of the house in contrast to the other columns of the dataset. This forces us to split our model into two arrays. We will use the values in one array $X$, representing the average area income, average area house age, average number of rooms in the area, average number of bedrooms and the area population to predict the price in $y$. Fundamentally, here we hypothesize that a linear relationship exists among the variables in either array. This sets the stage for linear regression.\n\n## Nailing down the basics\n\nLinear regression is a fundamental statistical technique that plays a pivotal role in data analysis, predictive modeling, and machine learning. It serves as a foundation for understanding the relationship between variables and making predictions based on that relationship. In this article, we will explore the mathematical background of linear regression, helping you grasp the core concepts behind this powerful tool.\n\nLinear regression aims to model the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. This equation takes the form:\n\\begin{equation}\n    \\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\ldots + b_n x_n + \\varepsilon\n\\end{equation}\nwhere $x_i$ is the $i^{th}$ independent variable, $b_i$ is the coefficient of the $i^{th}$ independent variable, and $\\varepsilon$ represents the error term i.e. the difference between the predicted and the actual values.\n\nThe goal of linear regression is to estimate the values of $b_1, b_2 \\ldots b_n$ and $a$ that best fit the data at hand. We can determine these values using the mean squared error estimation method. This can be done by to fit the data as follows:\n\n\\begin{equation}\n    \\min \\limits_{\\mathbf{b}} \\frac 1n S(\\mathbf{b}) = \\frac 1n \\sum \\limits_{i=1}^n (y_i - \\hat{y_i})^2 = \\frac 1n \\sum \\limits_{i=1}^n e_i^2,\n\\end{equation}\n\nwhere for each $1 \\leq i \\leq n$,\n\n\\begin{equation}\n    \\hat{y}_i = \\hat{b_0} + \\hat{b_1} x_{i1} + \\ldots + \\hat{b_n}x_{in}\n\\end{equation}\n\nNow, let $\\mathbf{e} = (e_i) \\in \\mathbb{R}$ and $\\hat{\\mathbf{y}} = (\\hat{y_i}) = \\mathbf{X \\hat{b}} \\in \\mathbb{R}^n$. Then $\\mathbb{e = y - \\hat{y}}$. This boils the above problem to the following:\n\n\\begin{equation}\n    \\min \\limits_{\\mathbf{\\hat{b}}} \\frac 1n S (\\mathbf{\\hat{b}}) = \\frac 1n ||\\mathbf{e}||^2 = \\frac 1n || \\mathbf{y - X \\hat{b}} ||^2.\n\\end{equation}\n\nBesides, this methodology, we shall apply two more methods to evaluate our regression problem:\n\n- Absolute Error (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|.$$\n- Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}.$$\n\nTogether, these three functions shall serve as our loss functions. However, as we have seen, the goal shall remain the same: We need to minimize the loss function.\n\n\nNow, let's split the data into a training set and a testing set. We will train our model on the training set and then use the test set to evaluate the model.\n\nHere we shall add a little bit of preprocessing by scaling and shifting the values in our dataset to constitute that of a standard normal distribution with mean 0 and variance 1. After this step we shall add them to our ```pipeline``` which shall help us plan out this entire study.\n\nHere, we shall invoke the ```LinearRegression()``` method that calls upon the linear regressor and have that trained on our dataset $X$ with our target values for $y$.\n\n\n\nNow, let's evaluate the model by checking out it's coefficients and how we can interpret them. We begin by printing our $y$-intercept, i.e. our value for $b_0$ as follows:\n\nThese are the values for $b_1, \\ldots, b_n$ which are our regression coefficients for average area's income, average area's house age, average areas number of rooms, average number of bedrooms and population for the area.\n\n Interpreting the coefficients:\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Income``` is associated with an increase of &#36;  21.52\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area House Age``` is associated with an increase of &#36; 164883.28.\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Number of Rooms``` is associated with an increase of &#36; 122368.67.\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Number of Bedrooms``` is associated with an increase of &#36; 2233.80.\n- Holding all other features fixed, a 1 unit increase in ```Area Population``` is associated with an increase of &#36; 15.15.\n\nNow that we have trained our model to the dataset and have obtained the values of $b_0 \\ldots b_n$, let's grab predictions off our test set and see how well it did!\n\n**Residual Histogram**\n\nLet's see how our errors pan out.\n\nLet's see the optimal values that we had for the Root Mean Square Error, Mean Squared Error and Mean Absolute Error when training as well as testing.\n\n## Ridge and Lasso Regression\n\nBefore diving into Ridge and Lasso, let's understand why we need these regularization techniques in the first place. In a standard linear regression model, we aim to find the coefficients for each predictor variable that minimize the sum of squared residuals. The problem arises when we have many predictor variables or when some of them are highly correlated. In such cases, the model tends to overfit, resulting in poor generalization to new, unseen data. Overfitting is a common problem in machine learning. It occurs when a model fits the training data so closely that it captures noise rather than the underlying relationships in the data. As a result, the model performs poorly on unseen data, a situation known as poor generalization.\n\nRegularization methods introduce a penalty term to the traditional least squares objective function, discouraging the model from assigning high values to the coefficients of predictor variables. Ridge and Lasso are two popular regularization techniques that help us tackle these issues. Ridge and Lasso regression are both linear regression techniques that combat overfitting by adding a regularization term to the linear regression cost function. This regularization term is based on Tikhonov regularization, named after the Russian mathematician Andrey Tikhonov. Tikhonov regularization, also known as L2 regularization, minimizes the sum of the squared coefficients, adding a penalty term to the cost function.\n\nThe cost function for ridge regression is expressed as follows:\n\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n b_i^2,\n\\end{equation}\n\nwhere $J(\\beta)$ is the cost function, $\\alpha$  is the regularization parameter that controls the strength of the regularization, $b_i$ are the model coefficients and MSE is the Mean Squared Error, measuring the model's error on the training data.\n\nOn the other hand, Lasso regression (Least Absolute Shrinkage and Selection Operator) uses L1 regularization, which minimizes the absolute values of the coefficients and can lead to sparsity (i.e., some coefficients being exactly zero). The Lasso cost function is expressed as:\n\n\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n |b_i|.\n\\end{equation}\n\n\n\nLets apply the theory above into practice by calling a ```Ridge``` object from ```scikit-learn``` for Ridge regression and fitting it to our dataset. We then evaluate the model using the same metrics of MAE, MSE, etc. and append it to a resultant dataframe for future reference.\n\nWe do the same for the Lasso regression model by calling in a ```Lasso``` object from ```scikit-learn``` for our needs.\n\n# Polynomial Regression\n\nWhile linear regression is effective for modeling simple relationships, many real-world scenarios exhibit more complex patterns. Polynomial regression addresses this by introducing higher-degree polynomial equations. The general form of a polynomial regression equation of degree can can be written as:\n\n\\begin{equation}\n    y = b_0 + b_1 X + b_2 X^2 + \\ldots + b_n X^n + \\varepsilon,\n\\end{equation}\n\nwhere $X_i$ is the $i^{th}$ independent variable, $b_i$ is the coefficient of the $i^{th}$ independent variable, and $\\varepsilon$ represents the error term i.e. the difference between the predicted and the actual values.\n\nJust like linear regression, polynomial regression models should be evaluated for their accuracy and performance. Techniques such as R-squared (coefficient of determination) and adjusted R-squared, as well as cross-validation methods, help assess the model's goodness of fit and prevent overfitting. The optimization problem remains the same as in the case of linear regression as we use the same loss functions.\n\nWe will put this theory into practice by transforming the features and raising them to their polynomial exponent and using a linear regressor to fit the data.\n\n# Models Comparison\n\n\n\nTherefore, in this blogpost we have learnt and compared against the Linear regression, the Ridge and Lasso regression, and polynomial regression.\n","srcMarkdownNoYaml":"\n\n\n\n\nRegression analysis is a basic yet powerful tool in the field of statistics and machine learning that allows us to probe in and model the the relationship between one or more independent variables and a dependent variable. Here, we shall take a look at linear and non-linear regression and the differences in performance.\n\nIn order to elucidate, we shall use the [USA Housing Prices dataset](https://www.kaggle.com/datasets/kanths028/usa-housing/) from Kaggle. However, before we get our hands dirty into the math and code, let us load our dataset and perform some exploratory data analysis.\n\n## Data Loading and EDA\n\nIn the python cells below, we shall call the necessary libraries to perform our analysis. Further, we shall first call our dataset and explore its characteristics to inspect it.\n\nLet us see now plot the data and see if there exists any pairwise relationship in our data.\n\n\n\n# A Linear Regression Model\n\nNow that we have inspected our data and have carried out our EDA, we shall begin to find ways to see if there exists any linear relationships between the data. Our primary focus of this dataset is the price. We need to see if there exists any relationship between the price of the house in contrast to the other columns of the dataset. This forces us to split our model into two arrays. We will use the values in one array $X$, representing the average area income, average area house age, average number of rooms in the area, average number of bedrooms and the area population to predict the price in $y$. Fundamentally, here we hypothesize that a linear relationship exists among the variables in either array. This sets the stage for linear regression.\n\n## Nailing down the basics\n\nLinear regression is a fundamental statistical technique that plays a pivotal role in data analysis, predictive modeling, and machine learning. It serves as a foundation for understanding the relationship between variables and making predictions based on that relationship. In this article, we will explore the mathematical background of linear regression, helping you grasp the core concepts behind this powerful tool.\n\nLinear regression aims to model the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. This equation takes the form:\n\\begin{equation}\n    \\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\ldots + b_n x_n + \\varepsilon\n\\end{equation}\nwhere $x_i$ is the $i^{th}$ independent variable, $b_i$ is the coefficient of the $i^{th}$ independent variable, and $\\varepsilon$ represents the error term i.e. the difference between the predicted and the actual values.\n\nThe goal of linear regression is to estimate the values of $b_1, b_2 \\ldots b_n$ and $a$ that best fit the data at hand. We can determine these values using the mean squared error estimation method. This can be done by to fit the data as follows:\n\n\\begin{equation}\n    \\min \\limits_{\\mathbf{b}} \\frac 1n S(\\mathbf{b}) = \\frac 1n \\sum \\limits_{i=1}^n (y_i - \\hat{y_i})^2 = \\frac 1n \\sum \\limits_{i=1}^n e_i^2,\n\\end{equation}\n\nwhere for each $1 \\leq i \\leq n$,\n\n\\begin{equation}\n    \\hat{y}_i = \\hat{b_0} + \\hat{b_1} x_{i1} + \\ldots + \\hat{b_n}x_{in}\n\\end{equation}\n\nNow, let $\\mathbf{e} = (e_i) \\in \\mathbb{R}$ and $\\hat{\\mathbf{y}} = (\\hat{y_i}) = \\mathbf{X \\hat{b}} \\in \\mathbb{R}^n$. Then $\\mathbb{e = y - \\hat{y}}$. This boils the above problem to the following:\n\n\\begin{equation}\n    \\min \\limits_{\\mathbf{\\hat{b}}} \\frac 1n S (\\mathbf{\\hat{b}}) = \\frac 1n ||\\mathbf{e}||^2 = \\frac 1n || \\mathbf{y - X \\hat{b}} ||^2.\n\\end{equation}\n\nBesides, this methodology, we shall apply two more methods to evaluate our regression problem:\n\n- Absolute Error (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|.$$\n- Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}.$$\n\nTogether, these three functions shall serve as our loss functions. However, as we have seen, the goal shall remain the same: We need to minimize the loss function.\n\n\nNow, let's split the data into a training set and a testing set. We will train our model on the training set and then use the test set to evaluate the model.\n\nHere we shall add a little bit of preprocessing by scaling and shifting the values in our dataset to constitute that of a standard normal distribution with mean 0 and variance 1. After this step we shall add them to our ```pipeline``` which shall help us plan out this entire study.\n\nHere, we shall invoke the ```LinearRegression()``` method that calls upon the linear regressor and have that trained on our dataset $X$ with our target values for $y$.\n\n\n\nNow, let's evaluate the model by checking out it's coefficients and how we can interpret them. We begin by printing our $y$-intercept, i.e. our value for $b_0$ as follows:\n\nThese are the values for $b_1, \\ldots, b_n$ which are our regression coefficients for average area's income, average area's house age, average areas number of rooms, average number of bedrooms and population for the area.\n\n Interpreting the coefficients:\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Income``` is associated with an increase of &#36;  21.52\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area House Age``` is associated with an increase of &#36; 164883.28.\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Number of Rooms``` is associated with an increase of &#36; 122368.67.\n- Holding all other features fixed, a 1 unit increase in ```Avg. Area Number of Bedrooms``` is associated with an increase of &#36; 2233.80.\n- Holding all other features fixed, a 1 unit increase in ```Area Population``` is associated with an increase of &#36; 15.15.\n\nNow that we have trained our model to the dataset and have obtained the values of $b_0 \\ldots b_n$, let's grab predictions off our test set and see how well it did!\n\n**Residual Histogram**\n\nLet's see how our errors pan out.\n\nLet's see the optimal values that we had for the Root Mean Square Error, Mean Squared Error and Mean Absolute Error when training as well as testing.\n\n## Ridge and Lasso Regression\n\nBefore diving into Ridge and Lasso, let's understand why we need these regularization techniques in the first place. In a standard linear regression model, we aim to find the coefficients for each predictor variable that minimize the sum of squared residuals. The problem arises when we have many predictor variables or when some of them are highly correlated. In such cases, the model tends to overfit, resulting in poor generalization to new, unseen data. Overfitting is a common problem in machine learning. It occurs when a model fits the training data so closely that it captures noise rather than the underlying relationships in the data. As a result, the model performs poorly on unseen data, a situation known as poor generalization.\n\nRegularization methods introduce a penalty term to the traditional least squares objective function, discouraging the model from assigning high values to the coefficients of predictor variables. Ridge and Lasso are two popular regularization techniques that help us tackle these issues. Ridge and Lasso regression are both linear regression techniques that combat overfitting by adding a regularization term to the linear regression cost function. This regularization term is based on Tikhonov regularization, named after the Russian mathematician Andrey Tikhonov. Tikhonov regularization, also known as L2 regularization, minimizes the sum of the squared coefficients, adding a penalty term to the cost function.\n\nThe cost function for ridge regression is expressed as follows:\n\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n b_i^2,\n\\end{equation}\n\nwhere $J(\\beta)$ is the cost function, $\\alpha$  is the regularization parameter that controls the strength of the regularization, $b_i$ are the model coefficients and MSE is the Mean Squared Error, measuring the model's error on the training data.\n\nOn the other hand, Lasso regression (Least Absolute Shrinkage and Selection Operator) uses L1 regularization, which minimizes the absolute values of the coefficients and can lead to sparsity (i.e., some coefficients being exactly zero). The Lasso cost function is expressed as:\n\n\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n |b_i|.\n\\end{equation}\n\n\n\nLets apply the theory above into practice by calling a ```Ridge``` object from ```scikit-learn``` for Ridge regression and fitting it to our dataset. We then evaluate the model using the same metrics of MAE, MSE, etc. and append it to a resultant dataframe for future reference.\n\nWe do the same for the Lasso regression model by calling in a ```Lasso``` object from ```scikit-learn``` for our needs.\n\n# Polynomial Regression\n\nWhile linear regression is effective for modeling simple relationships, many real-world scenarios exhibit more complex patterns. Polynomial regression addresses this by introducing higher-degree polynomial equations. The general form of a polynomial regression equation of degree can can be written as:\n\n\\begin{equation}\n    y = b_0 + b_1 X + b_2 X^2 + \\ldots + b_n X^n + \\varepsilon,\n\\end{equation}\n\nwhere $X_i$ is the $i^{th}$ independent variable, $b_i$ is the coefficient of the $i^{th}$ independent variable, and $\\varepsilon$ represents the error term i.e. the difference between the predicted and the actual values.\n\nJust like linear regression, polynomial regression models should be evaluated for their accuracy and performance. Techniques such as R-squared (coefficient of determination) and adjusted R-squared, as well as cross-validation methods, help assess the model's goodness of fit and prevent overfitting. The optimization problem remains the same as in the case of linear regression as we use the same loss functions.\n\nWe will put this theory into practice by transforming the features and raising them to their polynomial exponent and using a linear regressor to fit the data.\n\n# Models Comparison\n\n\n\nTherefore, in this blogpost we have learnt and compared against the Linear regression, the Ridge and Lasso regression, and polynomial regression.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"zephyr","title":"Linear and Non-Linear Regression: A Performance Comparison","description":"An introduction and performance comparison of the linear and non-linear regression techniques.","author":"Turbasu Chatterjee","date":"11/14/2023","categories":["class-project","machine-learning","regression"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}